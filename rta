#!/usr/bin/env python3
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import os
import time
from scipy.signal import resample

# Parameters
SCREEN_WIDTH = 1920
SCREEN_HEIGHT = 480
UPDATE_PERIOD = 0.25     # Update period in millisecond
SMOOTHING_ALPHA = 0.2   # Smoothing factor for FFT
SAMPLE_RATE = 48000
WINDOW_SIZE = 16384         # 340ms at 48kHz 
DPI = 100
N_TIME_BINS = int(SCREEN_HEIGHT * 0.9)
N_FREQ_BINS = (SCREEN_WIDTH - 100) // 2 + 1  # 100 pixels for the volume plot

# Update the dimensions of the fft_data and vol_data arrays
fft_data = np.zeros((N_TIME_BINS, N_FREQ_BINS))
vol_data = np.zeros(N_TIME_BINS)

def compute_smoothed_fft(signal, previous_fft=None, alpha=0.2):
    windowed_signal = signal * np.hanning(len(signal))
    freqs = np.fft.rfftfreq(len(signal), 1.0 / SAMPLE_RATE)
    fft_result = np.abs(np.fft.rfft(windowed_signal))

    # Smoothing with exponential moving average (EMA)
    if previous_fft is not None:
        smoothed_fft = alpha * fft_result + (1 - alpha) * previous_fft
    else:
        smoothed_fft = fft_result

    # Avoid log10 of zero or negative values by adding a small constant
    smoothed_fft = np.clip(smoothed_fft, a_min=1e-10, a_max=None)
    return freqs, 20 * np.log10(smoothed_fft)  # Return magnitude in dB

def compute_volume(signal):
    # Compute RMS (root mean square) volume of the signal
    return np.sqrt(np.mean(signal ** 2))

def setup_plot():
    global N_TIME_BINS, N_FREQ_BINS, SCREEN_WIDTH, SCREEN_HEIGHT, DPI
    global fft_data, vol_data

    figsize = (SCREEN_WIDTH / DPI, SCREEN_HEIGHT / DPI)
    fig, (ax_fft, ax_vol) = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [SCREEN_WIDTH - 100, 100]})
    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)  # Full window, no padding

    # FFT waterfall plot
    img_fft = ax_fft.imshow(fft_data, aspect='auto', origin='lower', cmap='inferno', extent=[40, 20000, 0, N_TIME_BINS])
    ax_fft.set_xlabel('Hz')
    ax_fft.set_xscale('log', base=2)

    def label_ticks(hz):
        if hz < 1000:
            return f'{hz:d}'
        k, c = divmod(hz, 1000)
        c = c // 100
        if c == 0:
            return f'{k:d}k'
        return f'{k:d}k{c:d}'

    ticks = [round(40 * 2 ** (i/3)) for i in range(27)] + [20000]
    ax_fft.set_xticks(ticks)
    ax_fft.set_xticklabels([label_ticks(t) for t in ticks])

    # Volume waterfall plot
    img_vol = ax_vol.imshow(vol_data[:, np.newaxis], aspect='auto', origin='lower', cmap='plasma', extent=[0, 1, 0, N_TIME_BINS])
    ax_vol.set_xticks([])  # No x-axis for volume
    ax_vol.set_ylabel('Time')

    return fig, ax_fft, ax_vol, img_fft, img_vol

def update_waterfall_plots(img_fft, img_vol, new_fft_data, new_volume_data):
    global fft_data, vol_data, N_TIME_BINS

    # Shift existing data upwards and append new FFT data at the bottom
    fft_data = np.roll(fft_data, -1, axis=0)
    fft_data[-1, :] = new_fft_data

    # Shift existing volume data upwards and append new volume at the bottom
    vol_data = np.roll(vol_data, -1)
    vol_data[-1] = new_volume_data

    # Update the image data for both plots
    img_fft.set_data(fft_data)
    img_vol.set_data(vol_data[:, np.newaxis])

    # Redraw the updated plot
    plt.draw()
    plt.pause(0.001)  # Small pause to allow plot to refresh smoothly

def process_audio(audio_buffer):
    previous_fft = None
    
    # Compute FFT and volume
    freqs, smoothed_fft = compute_smoothed_fft(audio_buffer, previous_fft, SMOOTHING_ALPHA)
    volume = compute_volume(audio_buffer)
    print(f'volume: {volume:.2f} dB')
    # Update the plots
    update_waterfall_plots(img_fft, img_vol, smoothed_fft, volume)
    previous_fft = smoothed_fft


def process_file(filepath):
    if not filepath.endswith('.wav'):
        raise ValueError('Only .wav files are supported')

    audio_data, sample_rate = sf.read(filepath)

    # Enforce fixed sample rate
    if sample_rate != SAMPLE_RATE:
        raise ValueError(f'Invalid sample rate: {sample_rate} Hz. Only {SAMPLE_RATE} Hz supported')

    # Convert to mono if necessary
    if len(audio_data.shape) == 2:
        audio_data = audio_data.mean(axis=1)

    audio_data = audio_data.astype(np.float32)  # Convert to float
    audio_data /= np.max(np.abs(audio_data))  # Normalize to range [-1, 1]
    return audio_data, len(audio_data)


def simulate_real_time_audio_input(filepath):
    global SAMPLE_RATE, WINDOW_SIZE
    audio_data, n_samples = process_file(filepath)

    t0 = time.time()
    max_read_pos = n_samples - WINDOW_SIZE
    while True:
        running_time = time.time() - t0
        position = max(0, min(int(running_time * SAMPLE_RATE) - WINDOW_SIZE, max_read_pos))
        if WINDOW_SIZE > n_samples:
            chunk = audio_data + [0] * (WINDOW_SIZE - n_samples)
        chunk = audio_data[position:position + WINDOW_SIZE]

        yield chunk
        if position + WINDOW_SIZE >= n_samples:
            break



if __name__ == '__main__':
    fig, ax_fft, ax_vol, img_fft, img_vol = setup_plot()
    try:
        while True:
            for f in os.listdir('testdata'):
                # Simulate real-time input by chunking the wav file                
                file_path = os.path.join('testdata', f)
                print(f'Processing {file_path}')
                for audio_chunk in simulate_real_time_audio_input(file_path):
                    process_audio(audio_chunk)


    except KeyboardInterrupt:
        print('Goodbye')
